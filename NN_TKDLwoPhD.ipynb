{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for number recognition\n",
    "How do we know if a Neural Network is working properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "\n",
    "training_images_file = 'gs://mist-public/train-images-idx3-ubyte'\n",
    "training_labels_file = 'gs://mist-public/train-labels-idx1-ubyte'\n",
    "validation_images_file = 'gs://mist-public/t10k-images-idx3-ubyte'\n",
    "validation_labels_file = 'gs://mist-public/t10k-images-idx1-ubyte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import os, re, math, json, shutil, pprint\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import IPython.display as display \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "print('Tensorflow version ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data.Dataset: parse files and prepare training and validation datasets\n",
    "Read https://www.tensorflow.org/guide/data : Best practices for building input pipelines with tf.data.Dataset\n",
    "# Resume\n",
    "Making data with tensor structure, this means that every array will become a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Function to read  image label\n",
    "def read_label(tf_bytestring):\n",
    "    label = tf.io.decode_raw(tf_bytestring, tf.uint8)\n",
    "    label = tf.reshape(label, [])\n",
    "    label = tf.one_hot(label, 10)\n",
    "    return label\n",
    "# Function for casting images into 28*28 pixels \n",
    "def read_image(tf_bytestring):\n",
    "    image = tf.io.decode_raw(tf_bytestring, tf.uint8)\n",
    "    image = tf.cast(image, tf.float32)/256.0\n",
    "    image = tf.reshape(image, [28*28])\n",
    "    return image\n",
    "\n",
    "# Function for loading image dataset and labels dataset\n",
    "def load_dataset(image_file, label_file):\n",
    "    imagedataset = tf.data.FixedLengthRecordDataset(image_file, 28*28, header_bytes = 16)\n",
    "    imagedataset = imagedataset.map(read_image, num_parallel_calls = 16)\n",
    "    labelsdataset = tf.data.FixedLengthRecordDataset(label_file,1 , header_bytes = 8)\n",
    "    labelsdataset = labelsdataset.map(read_label, num_parallel_calls = 16)\n",
    "    dataset = tf.data.Dataset.zip((imagedataset, labelsdataset))\n",
    "    return dataset\n",
    "\n",
    "#####--------------- Functions for Loading training and validation dataset------------############\n",
    "def get_training_dataset(image_file, label_file, batch_size):\n",
    "    dataset = load_dataset(image_file, label_file)\n",
    "    dataset = dataset.cache() # this small dataset can be entirely cached in RAM, for TPU this is important to get good performance from such a small dataset\n",
    "    dataset = dataset.shuffle(5000, reshuffle_each_iteration = True)\n",
    "    dataset = dataset.repeat() # mandatory for keras now \n",
    "    dataset = dataset.batch(batch_size, drop_remainder = True) # drop_reminder is important on TPU, batch size must be fixed\n",
    "    dataset = dataset.prefetch(AUTO) # fetch next batches while training on the current one (-1: autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(image_file, label_file):\n",
    "    dataset = load_dataset(image_file, label_file)\n",
    "    dataset = dataset.cache() # this small dataset can be entirely cached in RAM, for TPU this is important to get good performance from such a small dataset\n",
    "    dataset = dataset.batch(10000, drop_remainder = True) # 10000 items in eval dataset, all in one batch\n",
    "    dataset = dataset.repeat() # Mandatory for keras now\n",
    "    return dataset\n",
    "\n",
    "#instantiate the dataset\n",
    "training_dataset = get_training_dataset(training_images_file, training_labels_file, BATCH_SIZE)\n",
    "validation_dataset = get_validation_dataset(validation_images_file, validation_labels_file)\n",
    "\n",
    "# For TPU, we will need a function that returns the dataset\n",
    "training_input_fn = lambda: get_training_dataset(training_images_file, training_labels_file, BATCH_SIZE)\n",
    "validation_input_fn = lambda: get_validation_dataset(validation_images_file, validation_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
